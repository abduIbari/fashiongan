{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 352,
     "status": "ok",
     "timestamp": 1729162546315,
     "user": {
      "displayName": "Abdul Bari",
      "userId": "13730776251070988614"
     },
     "user_tz": -120
    },
    "id": "Zb7AMXVxlnMk"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aw-9boUp7ciD"
   },
   "source": [
    "# Visualising the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "id": "JEv1U2DIl4fB"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dataset = tfds.load(\"fashion_mnist\", split=\"train\", batch_size=batch_size, shuffle_files=True,  as_supervised=True)\n",
    "dataset = dataset.shuffle(buffer_size=1024)\n",
    "length = len(dataset)\n",
    "\n",
    "import random\n",
    "\n",
    "rand_batch_index = random.randint(0, length-1)\n",
    "selected_batch = dataset.skip(rand_batch_index).take(1)\n",
    "rand_image_index = random.randint(0, batch_size-1)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i+1)\n",
    "    for batch in selected_batch:\n",
    "        image_batch, label_batch = batch\n",
    "        ax.imshow(image_batch.numpy()[rand_image_index])\n",
    "        ax.axis(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "VguE9wPXl96V"
   },
   "outputs": [],
   "source": [
    "def scale_images(image, label):\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # Ensure the image is float before division\n",
    "    return image, label\n",
    "\n",
    "dataset = dataset.map(scale_images)\n",
    "dataset = dataset.take(1024)\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "dataset = dataset.repeat()\n",
    "dataset.as_numpy_iterator().next()[0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61IoT3t37hVx"
   },
   "source": [
    "# Generator NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "my67V0JCmAL5"
   },
   "outputs": [],
   "source": [
    "def build_genrator():\n",
    "    generator = tf.keras.Sequential([\n",
    "\n",
    "        # layer 1\n",
    "        tf.keras.layers.Dense(7*7*192, input_shape=[192], activation=\"leaky_relu\"),\n",
    "        tf.keras.layers.Reshape((7, 7, 192)),\n",
    "\n",
    "        # layer 2\n",
    "        tf.keras.layers.UpSampling2D(size=(2,2)),\n",
    "        tf.keras.layers.Conv2D(128, (5,5), padding=\"same\", activation=\"leaky_relu\"),\n",
    "\n",
    "        # layer 3\n",
    "        tf.keras.layers.UpSampling2D(),\n",
    "        tf.keras.layers.Conv2D(128, (5,5), padding=\"same\", activation=\"leaky_relu\"),\n",
    "\n",
    "        # layer 4\n",
    "        tf.keras.layers.Conv2D(64, (5,5), padding=\"same\", activation=\"leaky_relu\"),\n",
    "\n",
    "        # layer 5\n",
    "        tf.keras.layers.Conv2D(32, (4,4), padding=\"same\", activation=\"leaky_relu\"),\n",
    "\n",
    "        # layer 6\n",
    "        tf.keras.layers.Conv2D(1, (4,4), padding=\"same\", activation=\"sigmoid\"),\n",
    "\n",
    "    ])\n",
    "    return generator\n",
    "\n",
    "gen = build_genrator()\n",
    "gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5kbbWhu7PYL"
   },
   "source": [
    "### Layer 1\n",
    "In this layer, we create a dense layer where we input 192 random values. The nodes in the dense layer produce an output of size 7x7x192, which we will later reshape into the dimensions (7, 7, 192). This reshaping gives the output a spatial quality, marking the beginning of our generated image.\n",
    "\n",
    "The goal of the subsequent layers is to transform the dimensions from (7, 7, 192) to the dimensions of our fashion images, which are (28, 28, 1).\n",
    "\n",
    "### Layer 2\n",
    "We start by applying an upsampling layer that increases our spatial dimensions from (7, 7, 192) to (14, 14, 192). The size parameter indicates how much we want to upsample; in this case, (2, 2) means we are doubling both the number of rows and columns. It is important to note that this operation only increases spatial dimensions; it does not learn any features. The learning of features is accomplished by the Conv2D layer, where we specify 192 separate filters, each of size 5x5.\n",
    "\n",
    "### Layer 3\n",
    "In the next layer, we again upsample to further increase the spatial dimensions from (14, 14, 192) to (28, 28, 192). At this point, we have achieved our desired number of rows and columns. To learn features from this output, we again use a Conv2D layer with 192 filters, each of size 5x5.\n",
    "\n",
    "### Layer 4\n",
    "In this layer, there is no need to upsample, as we have already achieved the desired dimensions of 28x28. If we wanted, we could conclude the generator model here and use a single filter in our Conv2D layer. However, we do not do this because it would not have enough learning parameters to generate a sufficiently complex image. Therefore, we add additional Conv2D layers to enhance the model's capacity.\n",
    "\n",
    "*Note: The number of filters I used is arbitrary and can be adjusted based on the complexity needed for the generated images.*\n",
    "\n",
    "### Layer 5\n",
    "This layer consists of one more Conv2D layer to further increase the complexity of our generator, allowing it to produce a higher quality image.\n",
    "\n",
    "### Layer 6\n",
    "This is our final Conv2D layer, which consolidates all the learned knowledge into a single output channel. The activation function used here is sigmoid, as we want the pixel values to be between 0 and 1, enabling us to display a proper image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1Cta6NH7p_g"
   },
   "source": [
    "# Viewing the pretrained generated images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Q438O_8tmEnS"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i+1)\n",
    "    img = gen.predict(np.random.randn(1,192,1)) # 1 is the number of images that is generated (batch size), (192, 1) is the array of 192 random values that our generator expects\n",
    "    plt.imshow(np.squeeze(img))\n",
    "    plt.axis(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bu3XzgJb7wDd"
   },
   "source": [
    "# Discriminator NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "E-RMRikSmE-3"
   },
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    discriminator = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, (5,5), input_shape=[28,28,1], activation=\"leaky_relu\"),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Conv2D(128, (5,5), activation=\"leaky_relu\"),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Conv2D(128, (5,5), activation=\"leaky_relu\"),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Conv2D(64, (5,5), activation=\"leaky_relu\"),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Conv2D(32, (5,5), activation=\"leaky_relu\"),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    return discriminator\n",
    "\n",
    "disc = build_discriminator()\n",
    "disc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG7udkGq7zIC"
   },
   "source": [
    "The discriminator is essentially an image classifier; therefore, its architecture *resembles that of a standard CNN*.\n",
    "\n",
    "The first Conv2D layer expects an input of **(28, 28, 1)** because that is the output of our generator. Essentially, the discriminator receives the images generated by the generator and predicts whether they are generated images or not. *The Dropout layer helps us avoid overfitting by randomly deactivating a portion of the filters during training.* The input parameter of **0.25** indicates that 25% of the filters will be randomly deactivated during the forward pass.\n",
    "\n",
    "These types of layers are repeated multiple times, as seen in a typical CNN image classifier. In the end, the layers are flattened into a single dense layer, which is then connected to a single neuron that indicates whether the image is generated or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpK3uJaP77BA"
   },
   "source": [
    "# Custom training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MGdPLMecmHlO"
   },
   "outputs": [],
   "source": [
    "gen_opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "disc_opt = tf.keras.optimizers.Adam(learning_rate=0.00001 )\n",
    "\n",
    "gen_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "disc_loss = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "htViykQAmJqg"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class FashionGAN(Model):\n",
    "    def __init__(self, generator, discriminator, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def compile(self, gen_opt, disc_opt, gen_loss, disc_loss, *args, **kwargs):\n",
    "        super().compile(*args, **kwargs)\n",
    "\n",
    "        self.gen_opt = gen_opt\n",
    "        self.disc_opt = disc_opt\n",
    "        self.gen_loss = gen_loss\n",
    "        self.disc_loss = disc_loss\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        real_images = batch\n",
    "        generated_images = self.generator(tf.random.normal((batch_size, 192, 1)), training=False)\n",
    "\n",
    "        # Training the discriminator\n",
    "        with tf.GradientTape() as d_tape:\n",
    "            real_image_output = self.discriminator(real_images, training=True)\n",
    "            generated_image_output = self.discriminator(generated_images, training=True)\n",
    "            discriminator_predictions = tf.concat([real_image_output, generated_image_output], axis=0)\n",
    "\n",
    "            # Labels for real and fake image\n",
    "            discriminator_labels = tf.concat([tf.zeros_like(real_image_output), tf.ones_like(generated_image_output)], axis=0)\n",
    "\n",
    "            # Noise added to our labels\n",
    "            noise_real_image_output = 0.15*tf.random.uniform(tf.shape(real_image_output))\n",
    "            noise_generated_image_output = -0.15*tf.random.uniform(tf.shape(generated_image_output))\n",
    "            discriminator_labels += tf.concat([noise_real_image_output, noise_generated_image_output], axis=0)\n",
    "\n",
    "            # Loss calculation\n",
    "            total_disc_loss = self.disc_loss(discriminator_labels, discriminator_predictions)\n",
    "\n",
    "            #Backprop\n",
    "            disc_grads = d_tape.gradient(total_disc_loss, self.discriminator.trainable_variables)\n",
    "            self.disc_opt.apply_gradients(zip(disc_grads, self.discriminator.trainable_variables))\n",
    "\n",
    "        # Training the generator\n",
    "            with tf.GradientTape() as g_tape:\n",
    "                new_generated_images = self.generator(tf.random.normal((batch_size, 192, 1)), training=True)\n",
    "                generator_predictions = self.discriminator(new_generated_images, training=False)\n",
    "\n",
    "                total_gen_loss = self.gen_loss(tf.zeros_like(generator_predictions), generator_predictions)\n",
    "\n",
    "                gen_grads = g_tape.gradient(total_gen_loss, self.generator.trainable_variables)\n",
    "                self.gen_opt.apply_gradients(zip(gen_grads, self.generator.trainable_variables))\n",
    "\n",
    "            return {\"discriminator loss\": total_disc_loss,\"generator loss\": total_gen_loss}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YirQ3ic6Y3bA"
   },
   "source": [
    "In a custom training loop, we create a class that inherits from the base model class, which provides significant control over TensorFlow's behavior. This enables us to write a custom training loop tailored to our needs.\n",
    "\n",
    "There are three main methods that we need to define in this class: **init**, **compile**, and **train_step** (which replaces the standard fit method).\n",
    "\n",
    "In the **init** method, we pass in our discriminator and generator models, along with any additional arguments (args and kwargs) from the parent class. We then initialize both the generator and discriminator within this method.\n",
    "\n",
    "The **compile** method is where we pass the optimizers and loss functions that we defined for both the discriminator and generator, as well as any additional arguments from the parent class's compile method. This allows us to set up the optimizers and loss functions for training.\n",
    "\n",
    "In the **train_step** method, we take the current batch of data as input and initialize the real images. We then generate fake images using our generator, ensuring to set **training=False** since we are just predicting at this step. To train the models, we using tf.GradientTape API, which allows us to compute gradients with respect to all trainable variables. The training is then divided into two parts: **discriminator training** and **generator training**.\n",
    "\n",
    "### Discriminator Training\n",
    "First, we pass both the real and fake images (generated by the generator) through the discriminator, obtaining predictions for both. We then concatenate these predictions into a variable named **discriminator_predictions**. Next, we generate labels for the discriminator by concatenating a matrix of zeros (representing real images) and a matrix of ones (representing fake images). We assign real images a label of 0 and fake images a label of 1, as we want the discriminator to spot the fake images.\n",
    "\n",
    "To prevent the discriminator from becoming overconfident, we introduce some **noise** into the predictions. This technique smooths decision boundaries, prevents mode collapse, and improves generalization by giving better gradient signals to the generator.\n",
    "\n",
    "We compute the loss using the **disc_loss** function defined earlier. Finally, we calculate the gradients with respect to each trainable variable using the tf.GradientTape API, and the optimizer applies these gradients to update the discriminator’s parameters, minimizing the loss. This process pairs each gradient with its corresponding trainable variable for efficient backpropagation.\n",
    "\n",
    "### Generator training\n",
    "In the generator training phase, we begin by generating some fake images using the generator and setting **training=True**. We then pass these generated images through the discriminator, ensuring to set **training=False** here to prevent the discriminator from learning during the generator's training. The objective is for the generator to produce images that can successfully deceive the discriminator.\n",
    "\n",
    "Next, we calculate the generator’s loss using the **gen_loss** function. It's important to note that we want to reward the generator for successfully \"faking out\" the discriminator by classifying the generated images as real (with a label of 0). This encourages the generator to learn how to create more realistic images. Finally, similar to the discriminator, we calculate the gradients using tf.GradientTape, and the optimizer applies these gradients to update the generator’s parameters, enhancing the generator’s ability to produce convincing images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiMgSlfaZG4Y"
   },
   "source": [
    "# Custom Callback to Monitor Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "4fVlctkamK5l"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class ModelMonitor(Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=192, save_img_every=5, save_weights_every=150):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "        self.save_img_every = save_img_every\n",
    "        self.save_weights_every = save_weights_every\n",
    "        self.save_dir = '/Users/abdulbari/Desktop/ML/fashiongan'\n",
    "\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.makedirs(self.save_dir)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.save_img_every == 0:\n",
    "          random_latent_vectors = tf.random.uniform((self.num_img, self.latent_dim, 1))\n",
    "          generated_images = self.model.generator(random_latent_vectors)\n",
    "          generated_images *= 255\n",
    "          generated_images.numpy()\n",
    "          for i in range(self.num_img):\n",
    "              img = array_to_img(generated_images[i])\n",
    "              img.save(os.path.join(self.save_dir, f\"image_{epoch}_{i}.png\"))\n",
    "\n",
    "        if epoch % self.save_weights_every == 0:\n",
    "            model_save_path = os.path.join(self.save_dir, f\"gan_weights_epoch_{epoch}.weights.h5\")\n",
    "            self.model.generator.save_weights(model_save_path)\n",
    "            print(f\"Model weights saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfofT4A_Z14R"
   },
   "source": [
    "## Initialization Method\n",
    "In the __init__ method, we initialize key parameters for the monitoring process. The **num_img** parameter specifies the number of images to generate and save after every defined interval. The **latent_dim** parameter defines the dimensionality of the random latent vectors used for generating images. Additionally, **save_img_every** sets the frequency (in epochs) at which images are saved, while **save_weights_every** determines how often the model weights are saved during training. We also define a directory path, **save_dir**, where all generated images and saved weights will be stored. If this directory does not exist, it is created to ensure that the images and weights have a designated storage location.\n",
    "\n",
    "## On Epoch End Method\n",
    "The core functionality of the class lies in the **on_epoch_end** method, which is automatically called at the end of each training epoch.\n",
    "\n",
    "  1. Image Generation and Saving: If the current epoch is a multiple of **save_img_every**, the method generates a set of random latent vectors and uses the generator model to create synthetic images. These generated images are then scaled to a range of 0 to 255 (as images are typically represented) and saved in the specified directory with unique filenames that include the epoch number.\n",
    "\n",
    "  2. Weights Saving: Similarly, if the current epoch is a multiple of **save_weights_every**, the model's generator weights are saved to the specified directory with a filename that includes the epoch number. This enables the model to be restored or analyzed at various training stages, allowing for better tracking of the model's performance and improvements over time.\n",
    "\n",
    "By implementing this monitoring class, users can easily visualize the generator's progress and retain important model states throughout the training process, facilitating experimentation and debugging in the GAN training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "GiZbiGb6mMVA"
   },
   "outputs": [],
   "source": [
    "fashion = FashionGAN(gen, disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "xbUWO8lFJpye"
   },
   "outputs": [],
   "source": [
    "fashion.compile(gen_opt, disc_opt, gen_loss, disc_loss)\n",
    "\n",
    "steps_per_epoch = 60000 // batch_size\n",
    "history = fashion.fit(dataset, epochs=300, steps_per_epoch=steps_per_epoch , callbacks=[ModelMonitor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising the Trained Generated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.load_weights(\"/Users/abdulbari/Desktop/ML/fashion-gan/weights/gan_weights_epoch_300.weights.h5\")\n",
    "plt.figure(figsize=(7,7))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i+1)\n",
    "    img = gen.predict(tf.random.normal((1, 192, 1))) # 1 is the number of images that is generated (batch size), (192, 1) is the array of 192 random values that our generator expects\n",
    "    plt.imshow(np.squeeze(img))\n",
    "    plt.axis(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNn4ULB+ovtcObXwGJ132SZ",
   "gpuType": "T4",
   "mount_file_id": "1X0sNuS09SZ6y0R3pODCy-HJyUdNtpeB6",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
